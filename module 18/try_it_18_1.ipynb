{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2xoXLU7FKzC"
      },
      "source": [
        "### Comparing Models and Vectorization Strategies for Text Classification\n",
        "\n",
        "This Try-It activity focuses on weighing the positives and negatives of different estimators and vectorization strategies for a text classification problem.  In order to consider each of these components, you should make use of the `Pipeline` and `GridSearchCV` objects in Scikit-Learn to try different combinations of vectorizers with different estimators.  For each of these, you also want to use the `.cv_results_` to examine the time for the estimator to fit the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSrsszrUFKzE"
      },
      "source": [
        "### The Data\n",
        "\n",
        "The dataset below is from [kaggle]() and contains a dataset named the \"ColBert Dataset\" created for this [paper](https://arxiv.org/pdf/2004.12765.pdf).  You are to use the text column to classify whether or not the text was humorous.  It is loaded and displayed below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IhiOp74FKzF",
        "outputId": "63728410-4c3b-4ae5-a486-50ca9de37e9a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\prove\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\prove\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\prove\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yRUkiORQFKzG"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('data/dataset-minimal.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "9G6s9_ZfFKzG",
        "outputId": "fe7cd381-1e19-439d-a4a6-375ca158c57b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>humor</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Joe biden rules out 2020 bid: 'guys, i'm not r...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Watch: darvish gave hitter whiplash with slow ...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What do you call a turtle without its shell? d...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5 reasons the 2016 election feels so personal</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pasco police shot mexican migrant from behind,...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  humor\n",
              "0  Joe biden rules out 2020 bid: 'guys, i'm not r...  False\n",
              "1  Watch: darvish gave hitter whiplash with slow ...  False\n",
              "2  What do you call a turtle without its shell? d...   True\n",
              "3      5 reasons the 2016 election feels so personal  False\n",
              "4  Pasco police shot mexican migrant from behind,...  False"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ioZUGuIBFKzH"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['humor'], test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xW0-AzAkFKzH"
      },
      "outputs": [],
      "source": [
        "# Create a list of stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Create a stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i55jBqtqFKzH"
      },
      "source": [
        "#### Task\n",
        "\n",
        "\n",
        "**Text preprocessing:** As a pre-processing step, perform both `stemming` and `lemmatizing` to normalize your text before classifying. For each technique use both the `CountVectorize`r and `TfidifVectorizer` and use options for stop words and max features to prepare the text data for your estimator.\n",
        "\n",
        "**Classification:** Once you have prepared the text data with stemming lemmatizing techniques, consider `LogisticRegression`, `DecisionTreeClassifier`, and `MultinomialNB` as classification algorithms for the data. Compare their performance in terms of accuracy and speed.\n",
        "\n",
        "Share the results of your best classifier in the form of a table with the best version of each estimator, a dictionary of the best parameters and the best score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YdYWyW4hFKzI"
      },
      "outputs": [],
      "source": [
        "# Define a function to preprocess the text\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocesses text using stemming and lemmatization, stop word removal, lowercasing,\n",
        "    and punctuation removal.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to preprocess.\n",
        "\n",
        "    Returns:\n",
        "        str: The preprocessed text.\n",
        "    \"\"\"\n",
        "    text = text.lower()  # Lowercase\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    words = text.split()\n",
        "    words = [stemmer.stem(lemmatizer.lemmatize(word)) for word in words if word not in stop_words]\n",
        "    return ' '.join(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WSVsMPFEFKzJ"
      },
      "outputs": [],
      "source": [
        "# Define the pipelines\n",
        "pipelines = [\n",
        "    # CountVectorizer + Logistic Regression\n",
        "    Pipeline([\n",
        "        ('vectorizer', CountVectorizer(stop_words='english', max_features=1500, max_df=0.75)),\n",
        "        ('scaler', StandardScaler(with_mean=False)),\n",
        "        ('classifier', LogisticRegression(random_state=42))\n",
        "    ]),\n",
        "    # TfidfVectorizer + Logistic Regression\n",
        "    Pipeline([\n",
        "        ('vectorizer', TfidfVectorizer(stop_words='english', max_features=1500, max_df=0.75)),\n",
        "        ('scaler', StandardScaler(with_mean=False)),\n",
        "        ('classifier', LogisticRegression(random_state=42))\n",
        "    ]),\n",
        "    # CountVectorizer + Decision Tree\n",
        "    Pipeline([\n",
        "        ('vectorizer', CountVectorizer(stop_words='english', max_features=1500, max_df=0.75)),\n",
        "        ('scaler', StandardScaler(with_mean=False)),\n",
        "        ('classifier', DecisionTreeClassifier(random_state=42))\n",
        "    ]),\n",
        "    # TfidfVectorizer + Decision Tree\n",
        "    Pipeline([\n",
        "        ('vectorizer', TfidfVectorizer(stop_words='english', max_features=1500, max_df=0.75)),\n",
        "        ('scaler', StandardScaler(with_mean=False)),\n",
        "        ('classifier', DecisionTreeClassifier(random_state=42))\n",
        "    ]),\n",
        "    # CountVectorizer + Bayes\n",
        "    Pipeline([\n",
        "        ('vectorizer', CountVectorizer(stop_words='english', max_features=1500, max_df=0.75)),\n",
        "        ('scaler', StandardScaler(with_mean=False)),\n",
        "        ('classifier', MultinomialNB())\n",
        "    ]),\n",
        "    # TfidfVectorizer + Bayes\n",
        "    Pipeline([\n",
        "        ('vectorizer', TfidfVectorizer(stop_words='english', max_features=1500, max_df=0.75)),\n",
        "        ('scaler', StandardScaler(with_mean=False)),\n",
        "        ('classifier', MultinomialNB())\n",
        "    ]),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QP30QKveFKzJ"
      },
      "outputs": [],
      "source": [
        "param_grids = [\n",
        "    # CountVectorizer + Logistic Regression\n",
        "    {'classifier__C': [0.3, 0.4, 0.5]},\n",
        "    # TfidfVectorizer + Logistic Regression\n",
        "    {'classifier__C': [0.15, 0.2, 0.25]},\n",
        "    # CountVectorizer + Decision Tree\n",
        "    {'classifier__max_depth': [200, 220, 250], 'classifier__min_samples_split': [50, 60, 70]},\n",
        "    # TfidfVectorizer + Decision Tree\n",
        "    {'classifier__max_depth': [150, 175, 200], 'classifier__min_samples_split': [70, 80, 90]},\n",
        "    # CountVectorizer + Bayes\n",
        "    {'classifier__alpha': [0.004, 0.005, 0.006]},\n",
        "    # TfidfVectorizer + Bayes\n",
        "    {'classifier__alpha': [0.004, 0.005, 0.006]}\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvsUQWgTFKzJ"
      },
      "source": [
        "### Results will be in the format below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8_D7zrvBFKzK"
      },
      "outputs": [],
      "source": [
        "# Set up a dictionary to store the results\n",
        "results = {\n",
        "    'model': [\n",
        "        'Logistic Regression (CountVectorizer)',\n",
        "        'Logistic Regression (TfidfVectorizer)',\n",
        "        'Decision Tree (CountVectorizer)',\n",
        "        'Decision Tree (TfidfVectorizer)',\n",
        "        'Naive Bayes (CountVectorizer)',\n",
        "        'Naive Bayes (TfidfVectorizer)'\n",
        "    ],\n",
        "    'best_params': ['' for _ in range(6)],\n",
        "    'best_score': ['' for _ in range(6)],\n",
        "    'mean_fit_time': ['' for _ in range(6)]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "V3qQ8v5lFKzK",
        "outputId": "17ef7d34-410e-4795-8d8a-8aab079e94a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression (CountVectorizer):\n",
            "  Best params: {'classifier__C': 0.3}\n",
            "  Best cross-validation score: 0.8410\n",
            "  Mean fit time: 0.54s\n",
            "\n",
            "Logistic Regression (TfidfVectorizer):\n",
            "  Best params: {'classifier__C': 0.15}\n",
            "  Best cross-validation score: 0.8439\n",
            "  Mean fit time: 0.53s\n",
            "\n",
            "Decision Tree (CountVectorizer):\n",
            "  Best params: {'classifier__max_depth': 250, 'classifier__min_samples_split': 70}\n",
            "  Best cross-validation score: 0.7702\n",
            "  Mean fit time: 3.56s\n",
            "\n",
            "Decision Tree (TfidfVectorizer):\n",
            "  Best params: {'classifier__max_depth': 200, 'classifier__min_samples_split': 90}\n",
            "  Best cross-validation score: 0.7601\n",
            "  Mean fit time: 4.03s\n",
            "\n",
            "Naive Bayes (CountVectorizer):\n",
            "  Best params: {'classifier__alpha': 0.005}\n",
            "  Best cross-validation score: 0.8275\n",
            "  Mean fit time: 0.44s\n",
            "\n",
            "Naive Bayes (TfidfVectorizer):\n",
            "  Best params: {'classifier__alpha': 0.004}\n",
            "  Best cross-validation score: 0.8258\n",
            "  Mean fit time: 0.44s\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>best_params</th>\n",
              "      <th>best_score</th>\n",
              "      <th>mean_fit_time</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Logistic Regression (CountVectorizer)</th>\n",
              "      <td>{'classifier__C': 0.3}</td>\n",
              "      <td>0.841023</td>\n",
              "      <td>0.544951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Logistic Regression (TfidfVectorizer)</th>\n",
              "      <td>{'classifier__C': 0.15}</td>\n",
              "      <td>0.843861</td>\n",
              "      <td>0.533759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Decision Tree (CountVectorizer)</th>\n",
              "      <td>{'classifier__max_depth': 250, 'classifier__min_samples_split': 70}</td>\n",
              "      <td>0.770210</td>\n",
              "      <td>3.557001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Decision Tree (TfidfVectorizer)</th>\n",
              "      <td>{'classifier__max_depth': 200, 'classifier__min_samples_split': 90}</td>\n",
              "      <td>0.760110</td>\n",
              "      <td>4.030077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Naive Bayes (CountVectorizer)</th>\n",
              "      <td>{'classifier__alpha': 0.005}</td>\n",
              "      <td>0.827548</td>\n",
              "      <td>0.442591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Naive Bayes (TfidfVectorizer)</th>\n",
              "      <td>{'classifier__alpha': 0.004}</td>\n",
              "      <td>0.825785</td>\n",
              "      <td>0.443209</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                               best_params  \\\n",
              "model                                                                                                        \n",
              "Logistic Regression (CountVectorizer)                                               {'classifier__C': 0.3}   \n",
              "Logistic Regression (TfidfVectorizer)                                              {'classifier__C': 0.15}   \n",
              "Decision Tree (CountVectorizer)        {'classifier__max_depth': 250, 'classifier__min_samples_split': 70}   \n",
              "Decision Tree (TfidfVectorizer)        {'classifier__max_depth': 200, 'classifier__min_samples_split': 90}   \n",
              "Naive Bayes (CountVectorizer)                                                 {'classifier__alpha': 0.005}   \n",
              "Naive Bayes (TfidfVectorizer)                                                 {'classifier__alpha': 0.004}   \n",
              "\n",
              "                                       best_score  mean_fit_time  \n",
              "model                                                             \n",
              "Logistic Regression (CountVectorizer)    0.841023       0.544951  \n",
              "Logistic Regression (TfidfVectorizer)    0.843861       0.533759  \n",
              "Decision Tree (CountVectorizer)          0.770210       3.557001  \n",
              "Decision Tree (TfidfVectorizer)          0.760110       4.030077  \n",
              "Naive Bayes (CountVectorizer)            0.827548       0.442591  \n",
              "Naive Bayes (TfidfVectorizer)            0.825785       0.443209  "
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.set_option('display.max_colwidth', None)  # Set to None for unlimited width\n",
        "\n",
        "# Train and evaluate each pipeline with GridSearchCV\n",
        "for i, (pipeline, param_grid) in enumerate(zip(pipelines, param_grids)):\n",
        "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    results['best_params'][i] = str(grid_search.best_params_)\n",
        "    results['best_score'][i] = grid_search.best_score_\n",
        "    results['mean_fit_time'][i] = grid_search.cv_results_['mean_fit_time'][grid_search.best_index_]\n",
        "    print(f\"{results['model'][i]}:\")\n",
        "    print(f\"  Best params: {results['best_params'][i]}\")\n",
        "    print(f\"  Best cross-validation score: {results['best_score'][i]:.4f}\")\n",
        "    print(f\"  Mean fit time: {results['mean_fit_time'][i]:.2f}s\")\n",
        "    print()\n",
        "\n",
        "\n",
        "# Create a Pandas DataFrame from the results\n",
        "results_df = pd.DataFrame(results).set_index('model')\n",
        "\n",
        "# Print the results\n",
        "results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tIhixiOFKzK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
