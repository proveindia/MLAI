{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5f0288b403ed1232",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Required Assignment 9.4: LASSO and Sequential Feature Selection\n",
    "\n",
    "\n",
    "**Expected Time: 60 Minutes**\n",
    "\n",
    "**Total Points: 50**\n",
    "\n",
    "This assignment introduces the `Ridge` regression estimator from scikitlearn.  You will revisit the insurance data from the previous assignment and experiment with varying the `alpha` parameter discussed in Video 9.4. Your work here is a basic introduction where complexity in the preprocessing steps will be added to scale your data.  For now, you are just to familiarize yourself with the `Ridge` regression estimator and its `alpha` parameter. \n",
    "\n",
    "This assignment compares a second regularized regression method -- the LASSO -- with that of sequential feature selection.  The LASSO will be briefly discussed below, and you will use the scikit learn implementation.  Rather than using the LASSO as a model, you are to compare it to the `SequentialFeatureSelection` transformer as a method to select important features for a regression model. \n",
    "\n",
    "\n",
    "#### Index\n",
    "\n",
    "- [Problem 1](#Problem-1)\n",
    "- [Problem 2](#Problem-2)\n",
    "- [Problem 3](#Problem-3)\n",
    "- [Problem 4](#Problem-4)\n",
    "- [Problem 5](#Problem-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SequentialFeatureSelector, SelectFromModel\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn import set_config\n",
    "set_config(display=\"diagram\")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e92bc69b9408cf26",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### The Data\n",
    "\n",
    "For this exercise, you will revisit the automotive data.  The goal is again to predict the `mpg` column using the other numeric features.  You will build a polynomial model of degree 3 to compare the results of a `Lasso` and that of a `LinearRegression` model. Finally, you will use the `Lasso` estimator to select features in a pipeline with `SelectFromModel`. \n",
    "\n",
    "Below, the train and test data is created for you as `auto_X_train`, `auto_X_test`, `auto_y_train`, and `auto_y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data':        MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       " 0      8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       " 1      8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       " 2      7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       " 3      5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       " 4      3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       " ...       ...       ...       ...        ...         ...       ...       ...   \n",
       " 20635  1.5603      25.0  5.045455   1.133333       845.0  2.560606     39.48   \n",
       " 20636  2.5568      18.0  6.114035   1.315789       356.0  3.122807     39.49   \n",
       " 20637  1.7000      17.0  5.205543   1.120092      1007.0  2.325635     39.43   \n",
       " 20638  1.8672      18.0  5.329513   1.171920       741.0  2.123209     39.43   \n",
       " 20639  2.3886      16.0  5.254717   1.162264      1387.0  2.616981     39.37   \n",
       " \n",
       "        Longitude  \n",
       " 0        -122.23  \n",
       " 1        -122.22  \n",
       " 2        -122.24  \n",
       " 3        -122.25  \n",
       " 4        -122.25  \n",
       " ...          ...  \n",
       " 20635    -121.09  \n",
       " 20636    -121.21  \n",
       " 20637    -121.22  \n",
       " 20638    -121.32  \n",
       " 20639    -121.24  \n",
       " \n",
       " [20640 rows x 8 columns],\n",
       " 'target': 0        4.526\n",
       " 1        3.585\n",
       " 2        3.521\n",
       " 3        3.413\n",
       " 4        3.422\n",
       "          ...  \n",
       " 20635    0.781\n",
       " 20636    0.771\n",
       " 20637    0.923\n",
       " 20638    0.847\n",
       " 20639    0.894\n",
       " Name: MedHouseVal, Length: 20640, dtype: float64,\n",
       " 'frame':        MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       " 0      8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       " 1      8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       " 2      7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       " 3      5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       " 4      3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       " ...       ...       ...       ...        ...         ...       ...       ...   \n",
       " 20635  1.5603      25.0  5.045455   1.133333       845.0  2.560606     39.48   \n",
       " 20636  2.5568      18.0  6.114035   1.315789       356.0  3.122807     39.49   \n",
       " 20637  1.7000      17.0  5.205543   1.120092      1007.0  2.325635     39.43   \n",
       " 20638  1.8672      18.0  5.329513   1.171920       741.0  2.123209     39.43   \n",
       " 20639  2.3886      16.0  5.254717   1.162264      1387.0  2.616981     39.37   \n",
       " \n",
       "        Longitude  MedHouseVal  \n",
       " 0        -122.23        4.526  \n",
       " 1        -122.22        3.585  \n",
       " 2        -122.24        3.521  \n",
       " 3        -122.25        3.413  \n",
       " 4        -122.25        3.422  \n",
       " ...          ...          ...  \n",
       " 20635    -121.09        0.781  \n",
       " 20636    -121.21        0.771  \n",
       " 20637    -121.22        0.923  \n",
       " 20638    -121.32        0.847  \n",
       " 20639    -121.24        0.894  \n",
       " \n",
       " [20640 rows x 9 columns],\n",
       " 'target_names': ['MedHouseVal'],\n",
       " 'feature_names': ['MedInc',\n",
       "  'HouseAge',\n",
       "  'AveRooms',\n",
       "  'AveBedrms',\n",
       "  'Population',\n",
       "  'AveOccup',\n",
       "  'Latitude',\n",
       "  'Longitude'],\n",
       " 'DESCR': '.. _california_housing_dataset:\\n\\nCalifornia Housing dataset\\n--------------------------\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 20640\\n\\n:Number of Attributes: 8 numeric, predictive attributes and the target\\n\\n:Attribute Information:\\n    - MedInc        median income in block group\\n    - HouseAge      median house age in block group\\n    - AveRooms      average number of rooms per household\\n    - AveBedrms     average number of bedrooms per household\\n    - Population    block group population\\n    - AveOccup      average number of household members\\n    - Latitude      block group latitude\\n    - Longitude     block group longitude\\n\\n:Missing Attribute Values: None\\n\\nThis dataset was obtained from the StatLib repository.\\nhttps://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\\n\\nThe target variable is the median house value for California districts,\\nexpressed in hundreds of thousands of dollars ($100,000).\\n\\nThis dataset was derived from the 1990 U.S. census, using one row per census\\nblock group. A block group is the smallest geographical unit for which the U.S.\\nCensus Bureau publishes sample data (a block group typically has a population\\nof 600 to 3,000 people).\\n\\nA household is a group of people residing within a home. Since the average\\nnumber of rooms and bedrooms in this dataset are provided per household, these\\ncolumns may take surprisingly large values for block groups with few households\\nand many empty houses, such as vacation resorts.\\n\\nIt can be downloaded/loaded using the\\n:func:`sklearn.datasets.fetch_california_housing` function.\\n\\n.. rubric:: References\\n\\n- Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\\n  Statistics and Probability Letters, 33 (1997) 291-297\\n'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food = pd.read_csv('data/Pakistan_Food_Prices_2025.csv')\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing()\n",
    "fetch_california_housing(return_X_y=False, as_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1100 entries, 0 to 1099\n",
      "Data columns (total 67 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   Price_per_Kg                  1100 non-null   float64\n",
      " 1   Item_Banana                   1100 non-null   bool   \n",
      " 2   Item_Beef (Local)             1100 non-null   bool   \n",
      " 3   Item_Carrot                   1100 non-null   bool   \n",
      " 4   Item_Cheese (Local)           1100 non-null   bool   \n",
      " 5   Item_Chicken (Whole)          1100 non-null   bool   \n",
      " 6   Item_Chicken Breast           1100 non-null   bool   \n",
      " 7   Item_Chickpeas                1100 non-null   bool   \n",
      " 8   Item_Cooking Oil (Sunflower)  1100 non-null   bool   \n",
      " 9   Item_Eggplant                 1100 non-null   bool   \n",
      " 10  Item_Fish (Pomfret)           1100 non-null   bool   \n",
      " 11  Item_Fish (Rohu)              1100 non-null   bool   \n",
      " 12  Item_Flour (Atta)             1100 non-null   bool   \n",
      " 13  Item_Ghee                     1100 non-null   bool   \n",
      " 14  Item_Grapes                   1100 non-null   bool   \n",
      " 15  Item_Guava                    1100 non-null   bool   \n",
      " 16  Item_Lentils (Masoor)         1100 non-null   bool   \n",
      " 17  Item_Lentils (Moong)          1100 non-null   bool   \n",
      " 18  Item_Maize                    1100 non-null   bool   \n",
      " 19  Item_Mango                    1100 non-null   bool   \n",
      " 20  Item_Milk                     1100 non-null   bool   \n",
      " 21  Item_Mutton                   1100 non-null   bool   \n",
      " 22  Item_Onion                    1100 non-null   bool   \n",
      " 23  Item_Oranges                  1100 non-null   bool   \n",
      " 24  Item_Potato                   1100 non-null   bool   \n",
      " 25  Item_Rice (Basmati)           1100 non-null   bool   \n",
      " 26  Item_Rice (Irri)              1100 non-null   bool   \n",
      " 27  Item_Salt                     1100 non-null   bool   \n",
      " 28  Item_Spices (Mixed)           1100 non-null   bool   \n",
      " 29  Item_Spinach                  1100 non-null   bool   \n",
      " 30  Item_Sugar                    1100 non-null   bool   \n",
      " 31  Item_Tea (Loose)              1100 non-null   bool   \n",
      " 32  Item_Tomato                   1100 non-null   bool   \n",
      " 33  Item_Wheat                    1100 non-null   bool   \n",
      " 34  Item_Yogurt                   1100 non-null   bool   \n",
      " 35  Category_Condiment            1100 non-null   bool   \n",
      " 36  Category_Dairy                1100 non-null   bool   \n",
      " 37  Category_Fruit                1100 non-null   bool   \n",
      " 38  Category_Grain                1100 non-null   bool   \n",
      " 39  Category_Meat                 1100 non-null   bool   \n",
      " 40  Category_Oil                  1100 non-null   bool   \n",
      " 41  Category_Pulses               1100 non-null   bool   \n",
      " 42  Category_Vegetable            1100 non-null   bool   \n",
      " 43  City_Hyderabad                1100 non-null   bool   \n",
      " 44  City_Islamabad                1100 non-null   bool   \n",
      " 45  City_Karachi                  1100 non-null   bool   \n",
      " 46  City_Lahore                   1100 non-null   bool   \n",
      " 47  City_Multan                   1100 non-null   bool   \n",
      " 48  City_Peshawar                 1100 non-null   bool   \n",
      " 49  City_Quetta                   1100 non-null   bool   \n",
      " 50  City_Rawalpindi               1100 non-null   bool   \n",
      " 51  City_Sialkot                  1100 non-null   bool   \n",
      " 52  Month_August                  1100 non-null   bool   \n",
      " 53  Month_December                1100 non-null   bool   \n",
      " 54  Month_February                1100 non-null   bool   \n",
      " 55  Month_January                 1100 non-null   bool   \n",
      " 56  Month_July                    1100 non-null   bool   \n",
      " 57  Month_June                    1100 non-null   bool   \n",
      " 58  Month_March                   1100 non-null   bool   \n",
      " 59  Month_May                     1100 non-null   bool   \n",
      " 60  Month_November                1100 non-null   bool   \n",
      " 61  Month_October                 1100 non-null   bool   \n",
      " 62  Month_September               1100 non-null   bool   \n",
      " 63  Source_Municipal Report       1100 non-null   bool   \n",
      " 64  Source_Online Retailer        1100 non-null   bool   \n",
      " 65  Source_Retailer Listing       1100 non-null   bool   \n",
      " 66  Source_Wholesale Market       1100 non-null   bool   \n",
      "dtypes: bool(66), float64(1)\n",
      "memory usage: 79.6 KB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#get dummy for catagorial columns\n",
    "food = pd.get_dummies(food, drop_first=True)\n",
    "food.info()\n",
    "food.describe()\n",
    "food.isnull().sum()\n",
    "food = food.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'poly_features__degree': 1}\n",
      "Test MSE of best model:  6265.717250568826\n",
      "Pipeline(steps=[('poly_features',\n",
      "                 PolynomialFeatures(degree=1, include_bias=False)),\n",
      "                ('selector',\n",
      "                 SequentialFeatureSelector(estimator=LinearRegression(),\n",
      "                                           n_features_to_select=10)),\n",
      "                ('linreg', LinearRegression())])\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Item_Wheat=%{x}<br>Item_Banana=%{y}<br>Price_per_Kg=%{z}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#636efa",
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "",
         "scene": "scene",
         "showlegend": false,
         "type": "scatter3d",
         "x": [
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false
         ],
         "y": [
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          true,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          false,
          true,
          false
         ],
         "z": [
          188.93,
          866.6,
          243.67,
          121.4,
          127.83,
          1109.04,
          1010.15,
          65.6,
          281.27,
          192.97,
          1050.91,
          105.53,
          590.92,
          111.74,
          1281.94,
          127.77,
          84.96,
          120.82,
          70.29,
          747.99,
          771.03,
          582.56,
          216.45,
          238.43,
          131.91,
          179.05,
          1194.76,
          1280.22,
          814.16,
          244.61,
          982.68,
          254.18,
          120.63,
          154.55,
          163.29,
          711,
          235.43,
          105.04,
          68.81,
          643.06,
          910.93,
          43.91,
          343.24,
          1185.16,
          549.88,
          154.73,
          63.49,
          1406.41,
          115.59,
          611.47,
          959.24,
          217.14,
          349.83,
          222.18,
          957.43,
          859.05,
          91.38,
          1009.59,
          129.54,
          144.36,
          723.82,
          99.94,
          69.89,
          373.56,
          208.96,
          272.9,
          955.92,
          344.25,
          97.52,
          992.48,
          329.09,
          285.01,
          86.84,
          220.63,
          333.47,
          79.22,
          1046.05,
          246.38,
          744.03,
          126.58,
          124.5,
          147.71,
          176.24,
          71.91,
          146.81,
          82.15,
          130.16,
          1246.36,
          1130.89,
          108.14,
          985.21,
          94.3,
          252.05,
          796.94,
          1136.85,
          161.65,
          113.94,
          206.53,
          198.38,
          201.72,
          867.76,
          281.47,
          145.62,
          60.74,
          300.88,
          125.9,
          1023.75,
          241.28,
          113.16,
          75.26,
          138.75,
          113.19,
          746.1,
          109.03,
          72.05,
          134.58,
          245.89,
          327.5,
          108.73,
          744.81,
          97.28,
          116.8,
          82.11,
          205.12,
          1288.19,
          71.27,
          129.88,
          238.14,
          262.29,
          125.92,
          796.23,
          129.01,
          227.86,
          206.64,
          237.64,
          131.1,
          66.63,
          309.81,
          96.53,
          221.74,
          700.59,
          257.4,
          1202.58,
          229.35,
          151.88,
          46.38,
          220.81,
          1030.25,
          221.43,
          535.87,
          724.1,
          559.2,
          208.12,
          120.45,
          865.28,
          85.04,
          134.29,
          216.18,
          221.55,
          83.36,
          935.09,
          77.43,
          239.33,
          851.76,
          78.37,
          145.5,
          346.17,
          145.81,
          106.41,
          907.96,
          120.99,
          196.14,
          168.18,
          121.71,
          847.14,
          121.65,
          785.94,
          938.46,
          90.37,
          215.64,
          211.99,
          204.67,
          63.23,
          106.65,
          226.09,
          852.38,
          110.49,
          226.48,
          64.1,
          205.84,
          1304.09,
          210.52,
          754.34,
          52.81,
          284.04,
          126.74,
          125.69,
          125.19,
          272.98,
          42.47,
          281.36,
          65.7,
          92.93,
          200.51,
          243.61,
          218.35,
          1025.27,
          914.04,
          104.5,
          1014.03,
          127.37,
          721.32,
          517.21,
          109.58,
          528.86,
          255.91,
          958.53,
          997.38,
          122.71,
          249.84,
          870.1,
          211.03,
          641.19,
          100.21,
          145.2,
          491.19,
          1067.63,
          77.07,
          119.41,
          46.21,
          136.39,
          89.75,
          858.66,
          695.91,
          738.27,
          34.92,
          325.52,
          209.09,
          139.53,
          1186.36,
          274.08,
          127.39,
          136.4,
          102.99,
          287.66,
          114.98,
          107.69,
          114.43,
          224.25,
          238.7,
          143.31,
          203.71,
          240.98,
          170.49,
          276.99,
          127.61,
          779.72,
          942.3,
          663.02,
          124.32,
          74.49,
          972.32,
          235.78,
          115.34,
          311.04,
          1009.06,
          1027.18,
          223.75,
          218.55,
          124.76,
          132.45,
          486.69,
          93.81,
          190.34,
          214.39,
          220.74,
          870.47,
          39.55,
          192.02,
          121.13,
          76.15,
          63.76,
          705.93,
          207.01,
          279.47,
          879.59,
          1134.31,
          893.74,
          132.93,
          810.27,
          726.49,
          394.5,
          74.12,
          191.01,
          187.93,
          148.99,
          894.52,
          1099.82,
          89.35,
          112.78,
          114.06,
          117.5,
          264.43,
          781.32,
          133.96,
          122.63,
          154.14,
          119.37,
          66.86,
          994.02,
          289.77,
          901.3,
          145.62,
          114.37,
          89.36,
          211.62,
          227.21,
          150.37,
          1111.79,
          215.47,
          205.45,
          224.46,
          535.13,
          712.38,
          568.64,
          829.6,
          927.99,
          1018.59,
          631.91,
          114.77,
          830.6,
          143.72,
          119.94,
          230.35,
          117.46,
          62.84,
          101.76,
          326.84,
          764.14,
          191.77,
          966.79,
          129.35,
          117.65,
          127.43,
          112.42,
          119.91,
          703.81,
          344.63,
          103.92,
          254.88,
          163.4,
          1267.76,
          67.65,
          252.74,
          242.61,
          283.63,
          204.46,
          300.31,
          1130.31,
          111.93,
          914.23,
          353.03,
          858.41,
          133.23,
          214.29,
          936.02,
          198,
          574.28,
          91.07,
          829.85,
          78.66,
          135.64,
          817.23,
          180.23,
          724.37,
          62.78,
          349.36,
          261.07,
          186.42,
          132.21,
          157.44,
          36.98,
          897.31,
          188.97,
          70.17,
          127.23,
          129.72,
          940.64,
          1226.06,
          66.13,
          727.74,
          184.37,
          81.51,
          144.28,
          1235.88,
          215.54,
          674.45,
          210.99,
          375.19,
          1215.39,
          247.03,
          75.67,
          281.8,
          327.76,
          247,
          163.69,
          231.76,
          322.63,
          244.25,
          221.75,
          960.28,
          128.36,
          154.29,
          115.34,
          134.29,
          145.01,
          176.85,
          858.94,
          199.46,
          480.98,
          1224.05,
          59.97,
          107.63,
          893.86,
          728.58,
          106.4,
          387.23,
          251.34,
          65.96,
          744.38,
          215.96,
          134.06,
          87.96,
          220.89,
          144.68,
          135.71,
          419.19,
          824.25,
          125.34,
          89.68,
          960.71,
          1081.03,
          281.57,
          227.84,
          129.38,
          312.02,
          129.29,
          111.54,
          59.32,
          111.59,
          115.24,
          496.31,
          861,
          123.06,
          231.54,
          144.85,
          253.99,
          128.33,
          397.39,
          79.63,
          930.03,
          875.12,
          1104.78,
          490.55,
          720.63,
          135.27,
          108.48,
          118.35,
          70.75,
          927.84,
          123.77,
          93.97,
          206.71,
          110.06,
          86.92,
          977.54,
          1018.52,
          145.41,
          150.85,
          494.45,
          117.61,
          1302.9,
          863.94,
          875.36,
          132.76,
          55.72,
          82,
          126.29,
          907.98,
          330.09,
          653.88,
          330.8,
          234.02,
          126.55,
          1009.2,
          62.14,
          1073.9,
          153.73,
          814.18,
          358.87,
          216.4,
          281.49,
          119.67,
          183.59,
          163.75,
          1057.54,
          219.63,
          115.69,
          276.66,
          247.9,
          87.75,
          126.33,
          151.64,
          931.78,
          45.2,
          203.56,
          77.17,
          1077.05,
          286.48,
          777.87,
          929.79,
          115.21,
          111.14,
          199.57,
          991.62,
          127.83,
          569.76,
          208.48,
          813.93,
          187.9,
          71.95,
          178.62,
          854.06,
          91.03,
          212.41,
          106.1,
          91.25,
          1079.65,
          39.86,
          292.64,
          306.01,
          130.67,
          187.98,
          119.94,
          232.37,
          276.77,
          40.13,
          266.15,
          145.16,
          214.31,
          268.31,
          1358.35,
          95.01,
          68.45,
          108.58,
          97.2,
          109.98,
          213.45,
          85.04,
          917.57,
          166.23,
          142.33,
          133.71,
          99.03,
          206.98,
          117.54,
          1034.14,
          685.87,
          234.01,
          223.93,
          124.77,
          193.87,
          115.58,
          285.13,
          821.71,
          128.09,
          1154.17,
          117.63,
          197.77,
          632.67,
          233.73,
          301.77,
          221.64,
          58.68,
          227.89,
          840.79,
          810.62,
          215.79,
          216.26,
          150.91,
          146.61,
          273.82,
          308,
          711.13,
          630.28,
          912.99,
          262.89,
          141.22,
          1049.14,
          755.66,
          112.27,
          1028.53,
          286.74,
          109.19,
          255.8,
          1125.71,
          991.1,
          793.87,
          57.15,
          232.62,
          36.87,
          345.7,
          125.32,
          1257.53,
          471.42,
          126.17,
          103.53,
          1103.53,
          286.3,
          43.35,
          105.08,
          910.09,
          278.57,
          140.07,
          69.43,
          1152.35,
          515.34,
          1359.06,
          64.04,
          81.67,
          792.85,
          150.52,
          399.98,
          436.31,
          46.32,
          91.73,
          126.4,
          672.13,
          608.46,
          120.75,
          221.47,
          145.65,
          1169.88,
          1065.04,
          149.8,
          132.06,
          1136.26,
          330.64,
          914.3,
          143.85,
          241.01,
          112.71,
          228.23,
          206.49,
          282.33,
          140.49,
          251.19,
          339.82,
          291.4,
          111.34,
          535.02,
          103.57,
          286.05,
          418.22,
          1015.62,
          132.09,
          61.28,
          1119.33,
          196.8,
          125.02,
          129.24,
          876.29,
          106.22,
          105.49,
          978.39,
          813.03,
          305.68,
          1047.04,
          207.46,
          72.73,
          619.4,
          554.73,
          663.97,
          218.89,
          185.56,
          92.14,
          129.2,
          334.31,
          991.07,
          228.57,
          110.57,
          755.7,
          956.77,
          116.13,
          195.16,
          217.66,
          585.86,
          1178.68,
          229.74,
          134.29,
          999.98,
          868.04,
          834.83,
          53.38,
          829.06,
          58.93,
          154.42,
          73.81,
          82.45,
          951.1,
          832.63,
          921.15,
          1359.63,
          40.57,
          363.92,
          355.41,
          64.61,
          221.14,
          225.36,
          162.62,
          174.32,
          798.49,
          131.93,
          325.55,
          868.32,
          123.79,
          55.43,
          130.13,
          120.96,
          127.92,
          161.54,
          216.12,
          148.09,
          257.62,
          345.47,
          853.04,
          67.89,
          119.86,
          93.44,
          37.25,
          74.4,
          221,
          36.89,
          114.13,
          883.43,
          277.88,
          367.03,
          293.9,
          222.11,
          236.08,
          129.89,
          255.05,
          129.84,
          144.54,
          113.03,
          515.49,
          349.01,
          226.12,
          270.45,
          197.31,
          907.25,
          215.45,
          793.31,
          42.26,
          1304.86,
          223.89,
          37.06,
          1006.84,
          236.84,
          105.97,
          151.78,
          122.12,
          105.27,
          274.98,
          202.35,
          934.15,
          865.01,
          126.51,
          856.6,
          143.66,
          234.31,
          974.62,
          102.87,
          1017.77,
          112.13,
          131.96,
          251.71,
          91.14,
          684.9,
          121.44,
          221.05,
          843.19,
          83.99,
          827.79,
          850.96,
          104.91,
          137.15,
          109.16,
          695.33,
          180.44,
          274.79,
          218.63,
          118.51,
          899.59,
          126.44,
          238.99,
          316.85,
          620.44,
          494.87,
          975.55,
          57.41,
          134.39,
          77.34,
          75.44,
          125.39,
          221.82,
          228.73,
          137.97,
          850.04,
          225.74,
          943.09,
          39.02,
          857.35,
          211.65,
          38.86,
          39.01,
          183.83,
          66.29,
          117,
          233.49,
          114.95,
          189.75,
          973.35,
          110.28,
          865.41,
          96.22,
          627.49,
          142.05,
          795.19,
          138.27,
          125.28,
          64.07,
          1271.27,
          44.41,
          751.87,
          128.87,
          244.94,
          314.41,
          370.07,
          206,
          571.31,
          126.45,
          122.57,
          53,
          182.38,
          211.44,
          105.52,
          402.76,
          686.38,
          90.63,
          148.97,
          120.36,
          139.19,
          128.96,
          334.68,
          275.05,
          123.33,
          842.44,
          344.42,
          1304.6,
          45.5,
          1024.05,
          1000.55,
          87.84,
          196.1,
          64.17,
          787.89,
          238.06,
          177.34,
          61.73,
          84.06,
          191.75,
          43.74,
          122.53,
          1046.5,
          899.01,
          363.83,
          75.21,
          42.72,
          126.62,
          287.15,
          68.62,
          985.43,
          513.56,
          401.97,
          62.21,
          405.87,
          67.59,
          153.09,
          78.83,
          323,
          604.41,
          216.67,
          142.84,
          253.9,
          720.95,
          128.9,
          114.94,
          154.77,
          192.56,
          636.71,
          977.75,
          314.84,
          117.93,
          110.66,
          1053.09,
          888.46,
          45.54,
          238.86,
          60.48,
          255,
          885.88,
          242.04,
          1156.12,
          312.45,
          131.5,
          40.37,
          948.91,
          135.01,
          257.06,
          76.89,
          115.64,
          85.63,
          144.71,
          115.83,
          135.76,
          954.71,
          1112.03,
          65.44,
          241.84,
          666.04,
          920.28,
          63.96,
          115.3,
          597.47,
          246.71,
          1042.64,
          1022.66,
          1092.46,
          121.62,
          121.15,
          242.64,
          78.89,
          136.44,
          119.09,
          341.05,
          154.73,
          79.35,
          65.36,
          357.94,
          536.93,
          164.08,
          226.72,
          232.72,
          273.85,
          36,
          795.68,
          822.7,
          886.32,
          289.26,
          108.22,
          109.44,
          60.14,
          83.33,
          232.62,
          344.54,
          97.04,
          835.89,
          737.41,
          110.52,
          297.49,
          81.86,
          174.91,
          320.5,
          990.48,
          130.44,
          243.66,
          380.05,
          189.88,
          141.14,
          100.18,
          90.5,
          513.38,
          301.52,
          121.06,
          133.82,
          189.52,
          875.31,
          119.09,
          62.79,
          826.97,
          107.21,
          70.26,
          530.88,
          102.07,
          853.77,
          134.96,
          154.25,
          161.38,
          133.98,
          80.24,
          131.67,
          62.15,
          921.48,
          208.44,
          263.43,
          612.31,
          708.19,
          1185.64,
          1022.33,
          1315.1,
          899.6,
          93.25,
          1005.25,
          131.07,
          82.39,
          277.58,
          133.44,
          99.86,
          213.35,
          492.86,
          108.98,
          199.87,
          285.17,
          127.52,
          690.06,
          142.53,
          101,
          321.58,
          79.84,
          1018.51,
          1080.25,
          230.01,
          152.14,
          884.36,
          636.97,
          103.46,
          921.13,
          137.32,
          161.06,
          548.45,
          104.04,
          265.49,
          120.43,
          306.61,
          800.43,
          111.67,
          673.99,
          698.56,
          92.92,
          212.84,
          275.04,
          58.8,
          131.71,
          218.29,
          897.65,
          596.08,
          1109.79,
          864.24,
          134.98,
          786.38,
          116.51,
          201.63,
          872.88,
          34.21,
          842.18,
          238.91,
          119.9,
          197.52,
          139.01,
          293.72,
          834.71,
          105.82,
          255.39,
          126.23,
          368.42,
          122.15,
          127.05,
          312.53,
          86.92,
          200.92,
          201.47,
          920.87,
          1043.6,
          928.38,
          113.06,
          112.63,
          74.99,
          134.92,
          1136.56
         ]
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "scene": {
         "domain": {
          "x": [
           0,
           1
          ],
          "y": [
           0,
           1
          ]
         },
         "xaxis": {
          "title": {
           "text": "Item_Wheat"
          }
         },
         "yaxis": {
          "title": {
           "text": "Item_Banana"
          }
         },
         "zaxis": {
          "title": {
           "text": "Price_per_Kg"
          }
         }
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "3D Scatter plot of Wheat, Rice and Total Price"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must be the same size",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     30\u001b[39m ols_preds = best_model.predict(X)\n\u001b[32m     31\u001b[39m Xnp = X.to_numpy()\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[43mplt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXnp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m plt.plot(Xnp, ols_preds, \u001b[33m'\u001b[39m\u001b[33mr--\u001b[39m\u001b[33m'\u001b[39m, label = \u001b[33m'\u001b[39m\u001b[33mols predictions\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     34\u001b[39m plt.legend()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Study\\anaconda3\\Lib\\site-packages\\matplotlib\\_api\\deprecation.py:453\u001b[39m, in \u001b[36mmake_keyword_only.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    447\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > name_idx:\n\u001b[32m    448\u001b[39m     warn_deprecated(\n\u001b[32m    449\u001b[39m         since, message=\u001b[33m\"\u001b[39m\u001b[33mPassing the \u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%(obj_type)s\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    450\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpositionally is deprecated since Matplotlib \u001b[39m\u001b[38;5;132;01m%(since)s\u001b[39;00m\u001b[33m; the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    451\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mparameter will become keyword-only in \u001b[39m\u001b[38;5;132;01m%(removal)s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    452\u001b[39m         name=name, obj_type=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m453\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Study\\anaconda3\\Lib\\site-packages\\matplotlib\\pyplot.py:3939\u001b[39m, in \u001b[36mscatter\u001b[39m\u001b[34m(x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, colorizer, plotnonfinite, data, **kwargs)\u001b[39m\n\u001b[32m   3919\u001b[39m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes.scatter)\n\u001b[32m   3920\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mscatter\u001b[39m(\n\u001b[32m   3921\u001b[39m     x: \u001b[38;5;28mfloat\u001b[39m | ArrayLike,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3937\u001b[39m     **kwargs,\n\u001b[32m   3938\u001b[39m ) -> PathCollection:\n\u001b[32m-> \u001b[39m\u001b[32m3939\u001b[39m     __ret = \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3940\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3941\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3942\u001b[39m \u001b[43m        \u001b[49m\u001b[43ms\u001b[49m\u001b[43m=\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmarker\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmarker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3946\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3947\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvmin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3948\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvmax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3949\u001b[39m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3950\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlinewidths\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlinewidths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3951\u001b[39m \u001b[43m        \u001b[49m\u001b[43medgecolors\u001b[49m\u001b[43m=\u001b[49m\u001b[43medgecolors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3952\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolorizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolorizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3953\u001b[39m \u001b[43m        \u001b[49m\u001b[43mplotnonfinite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mplotnonfinite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3954\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3955\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3956\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3957\u001b[39m     sci(__ret)\n\u001b[32m   3958\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m __ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Study\\anaconda3\\Lib\\site-packages\\matplotlib\\_api\\deprecation.py:453\u001b[39m, in \u001b[36mmake_keyword_only.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    447\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > name_idx:\n\u001b[32m    448\u001b[39m     warn_deprecated(\n\u001b[32m    449\u001b[39m         since, message=\u001b[33m\"\u001b[39m\u001b[33mPassing the \u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%(obj_type)s\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    450\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpositionally is deprecated since Matplotlib \u001b[39m\u001b[38;5;132;01m%(since)s\u001b[39;00m\u001b[33m; the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    451\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mparameter will become keyword-only in \u001b[39m\u001b[38;5;132;01m%(removal)s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    452\u001b[39m         name=name, obj_type=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m453\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Study\\anaconda3\\Lib\\site-packages\\matplotlib\\__init__.py:1521\u001b[39m, in \u001b[36m_preprocess_data.<locals>.inner\u001b[39m\u001b[34m(ax, data, *args, **kwargs)\u001b[39m\n\u001b[32m   1518\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m   1519\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(ax, *args, data=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m   1520\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1521\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[43m            \u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1523\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcbook\u001b[49m\u001b[43m.\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1524\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbook\u001b[49m\u001b[43m.\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1526\u001b[39m     bound = new_sig.bind(ax, *args, **kwargs)\n\u001b[32m   1527\u001b[39m     auto_label = (bound.arguments.get(label_namer)\n\u001b[32m   1528\u001b[39m                   \u001b[38;5;129;01mor\u001b[39;00m bound.kwargs.get(label_namer))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Study\\anaconda3\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:4900\u001b[39m, in \u001b[36mAxes.scatter\u001b[39m\u001b[34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, colorizer, plotnonfinite, **kwargs)\u001b[39m\n\u001b[32m   4898\u001b[39m y = np.ma.ravel(y)\n\u001b[32m   4899\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x.size != y.size:\n\u001b[32m-> \u001b[39m\u001b[32m4900\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mx and y must be the same size\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4902\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m s \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4903\u001b[39m     s = (\u001b[32m20\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mpl.rcParams[\u001b[33m'\u001b[39m\u001b[33m_internal.classic_mode\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[32m   4904\u001b[39m          mpl.rcParams[\u001b[33m'\u001b[39m\u001b[33mlines.markersize\u001b[39m\u001b[33m'\u001b[39m] ** \u001b[32m2.0\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: x and y must be the same size"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#generate train/test data for auto\n",
    "sequential_pipe = Pipeline([('poly_features', PolynomialFeatures(degree = 1, include_bias = False)),\n",
    "                           ('selector', SequentialFeatureSelector(LinearRegression(), \n",
    "                                                                  n_features_to_select=10)),\n",
    "                           ('linreg', LinearRegression())])\n",
    "\n",
    "model_finder = GridSearchCV(sequential_pipe, \n",
    "                            param_grid = {'poly_features__degree': [1]},\n",
    "                            cv = 2,\n",
    "                            scoring = 'neg_mean_squared_error',\n",
    "                            n_jobs = -1)\n",
    "\n",
    "X = food[['Item_Banana', 'Item_Beef (Local)', 'Item_Carrot', 'Item_Cheese (Local)', 'Item_Chicken (Whole)', 'Item_Chicken Breast', 'Item_Chickpeas', 'Item_Cooking Oil (Sunflower)', 'Item_Eggplant', 'Item_Fish (Pomfret)', 'Item_Fish (Rohu)', 'Item_Flour (Atta)', 'Item_Ghee', 'Item_Grapes', 'Item_Guava', 'Item_Lentils (Masoor)', 'Item_Lentils (Moong)', 'Item_Maize', 'Item_Mango', 'Item_Milk', 'Item_Mutton', 'Item_Onion', 'Item_Oranges', 'Item_Potato', 'Item_Rice (Basmati)', 'Item_Rice (Irri)', 'Item_Salt', 'Item_Spices (Mixed)', 'Item_Spinach', 'Item_Sugar', 'Item_Tea (Loose)', 'Item_Tomato', 'Item_Wheat', 'Item_Yogurt', 'Category_Condiment', 'Category_Dairy', 'Category_Fruit', 'Category_Grain', 'Category_Meat', 'Category_Oil', 'Category_Pulses', 'Category_Vegetable', 'City_Hyderabad', 'City_Islamabad', 'City_Karachi', 'City_Lahore', 'City_Multan', 'City_Peshawar', 'City_Quetta', 'City_Rawalpindi', 'City_Sialkot', 'Month_August', 'Month_December', 'Month_February', 'Month_January', 'Month_July', 'Month_June', 'Month_March', 'Month_May', 'Month_November', 'Month_October', 'Month_September', 'Source_Municipal Report', 'Source_Online Retailer', 'Source_Retailer Listing', 'Source_Wholesale Market']]\n",
    "y = food['Price_per_Kg'] \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)   \n",
    "model_finder.fit(X_train, y_train)\n",
    "print(\"Best parameters found: \", model_finder.best_params_)\n",
    "best_model = model_finder.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Test MSE of best model: \", mse)\n",
    "print(best_model)\n",
    "fig = px.scatter_3d(food, x='Item_Wheat', y='Item_Banana', z='Price_per_Kg', title='3D Scatter plot of Wheat, Rice and Total Price')\n",
    "fig.show()\n",
    "\n",
    "# Uncomment to visualize solutions\n",
    "ols_preds = best_model.predict(X)\n",
    "Xnp = X.to_numpy()\n",
    "plt.scatter(Xnp, y, label = 'data')\n",
    "plt.plot(Xnp, ols_preds, 'r--', label = 'ols predictions')\n",
    "plt.legend()\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-add79ff14f85c4b3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "\n",
    "### Problem 1\n",
    "\n",
    "#### The auto data\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "To start, build a `Pipeline` named `auto_pipe` with named steps `polyfeatures`, `scaler` and `lasso` model that utilizes `PolynomialFeatures`, `StandardScaler`, and the `Lasso` estimator with the following parameters:\n",
    "\n",
    "- `degree = 3` in `PolynomialFeatures`\n",
    "- `include_bias = False` in `PolynomialFeatures`\n",
    "- `random_state = 42` in `Lasso`\n",
    "\n",
    "Fit the pipeline on `auto_X_train` and `auto_y_train` data given.  Extract the lasso coefficients from the pipeline and assign them as an array to `lasso_coefs` below.  \n",
    "\n",
    "**HINT**: Use the `.named_steps['lasso']` to extract that lasso estimator and use the `.coef_` attribute after fitting to access the model coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4b2351ce5efba3ad",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "auto_pipe = ''\n",
    "lasso_coefs = ''\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "auto_pipe = Pipeline([('polyfeatures', PolynomialFeatures(degree = 3, include_bias = False)),\n",
    "                      ('scaler', StandardScaler()),\n",
    "                     ('lasso', Lasso(random_state = 42))])\n",
    "auto_pipe.fit(auto_X_train, auto_y_train)\n",
    "lasso_coefs = auto_pipe.named_steps['lasso'].coef_\n",
    "### END SOLUTION\n",
    "\n",
    "# Answer check\n",
    "print(type(lasso_coefs))\n",
    "print(lasso_coefs)\n",
    "auto_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-00d34c3ff5947d86",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "\n",
    "### Problem 2\n",
    "\n",
    "#### Error in `Lasso` model\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "Now, compute the mean squared error of the LASSO model on both the train and test data, `auto_X_train` and `auto_X_test`, respectively.  Assign this as a float to `lasso_train_mse` and `lasso_test_mse`, respectively.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-27bc7ac17713b3d7",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "lasso_train_mse = ''\n",
    "lasso_test_mse = ''\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "lasso_train_mse = mean_squared_error(auto_y_train, auto_pipe.predict(auto_X_train))\n",
    "lasso_test_mse = mean_squared_error(auto_y_test, auto_pipe.predict(auto_X_test))\n",
    "### END SOLUTION\n",
    "\n",
    "# Answer check\n",
    "print(lasso_train_mse)\n",
    "print(lasso_test_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6cd009f6888369f8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "\n",
    "### Problem 3\n",
    "\n",
    "#### Non-zero coefficients\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "Using the `lasso_coefs` determine the number of features with non-zero coefficients and determine the name of those features as a result of the polynomial feature transformation.  \n",
    "\n",
    "To do this, access the `named_steps['polyfeatures']` feature from the `auto_pipe` pipeline and chain the `get_feature_names_out()` to get the features name. Assign the result to `feature_names`.\n",
    "\n",
    "Next, create a DataFrame named `lasso_df` below that has two columns -- `feature` and `coef`.  To the `feature` column assign `feature_names`. To the `coef` column assign `lasso_coefs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e5ed130aa3396617",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "feature_names = ''\n",
    "lasso_df = ''\n",
    "\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "feature_names = auto_pipe.named_steps['polyfeatures'].get_feature_names_out()\n",
    "lasso_df = pd.DataFrame({'feature': feature_names, 'coef': lasso_coefs})\n",
    "### END SOLUTION\n",
    "\n",
    "# Answer check\n",
    "print(type(feature_names))\n",
    "lasso_df.loc[lasso_df['coef'] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-af9586e34b8d17e2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "\n",
    "### Problem 4\n",
    "\n",
    "#### Comparing `Lasso` to `SequentialFeatureSelection`\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "As seen above, the Lasso model effectively eliminated all but 6 features from the cubic polynomial example.  Now, you are to build a `Pipeline` object called `sequential_pipe` with named steps `poly_features`, `selector`, and `linreg` with `PolynomialFeatures`, `SequentialFeatureSelector`, and `LinearRegression` respectively that uses the folowing parameters:\n",
    "\n",
    "- `degree = 3` in `PolynomialFeatures` step `poly_features`\n",
    "- `include_bias = False` in `PolynomialFeatures` step `poly_features`\n",
    "- `n_features_to_select = 6` in `selector`\n",
    "\n",
    "Assign this pipeline object to `sequential_pipe`.\n",
    "\n",
    "Next, use the `fit` function on `scaled_pipe` to train your model on `auto_X_train` and `auto_y_train`. \n",
    "\n",
    "Use the `mean_squared_error` function to compute the MSE between `auto_y_train` and` sequential_pipe.predict(auto_X_train)`. Assign your result to `sequential_train_mse`.\n",
    "\n",
    "Use the `mean_squared_error` function to compute the MSE between `auto_y_test` and `sequential_pipe.predict(auto_X_test)`. Assign your result to `sequential_test_mse`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-113e81ee2e4d2705",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "sequential_pipe = ''\n",
    "sequential_train_mse = ''\n",
    "sequential_test_mse = ''\n",
    "\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "sequential_pipe = Pipeline([('poly_features', PolynomialFeatures(degree = 3, include_bias = False)),\n",
    "                           ('selector', SequentialFeatureSelector(LinearRegression(), \n",
    "                                                                  n_features_to_select=6)),\n",
    "                           ('linreg', LinearRegression())])\n",
    "sequential_pipe.fit(auto_X_train, auto_y_train)\n",
    "sequential_train_mse = mean_squared_error(auto_y_train, sequential_pipe.predict(auto_X_train))\n",
    "sequential_test_mse = mean_squared_error(auto_y_test, sequential_pipe.predict(auto_X_test))\n",
    "### END SOLUTION\n",
    "\n",
    "# Answer check\n",
    "print(sequential_train_mse)\n",
    "print(sequential_test_mse)\n",
    "sequential_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4b1766f9492c5fec",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "\n",
    "### Problem 5\n",
    "\n",
    "#### Using `Lasso` as a feature selector\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "Rather than using the `Lasso` as the estimator, you can use the results of the `Lasso` to select features that are subsequently used in a `LinearRegression` estimator.  To do so, scikitlearn provides a function in the `feature_selection` module called `SelectFromModel` that will select the features based on coefficients.  \n",
    "\n",
    "As such, using the `Lasso` estimator to select features would involve instantiating the `SelectFromModel` transformer and selecting features as:\n",
    "\n",
    "```python\n",
    "selector = SelectFromModel(Lasso())\n",
    "selector.transform(auto_X_train)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3406cfb052fec8d2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "From here, the selector can be used in a `Pipeline` after transforming the features and before building a regression model.  Such a pipeline is given below and you are to use this to fit on the training data and score on the testing data.  Which model performs better, the model with sequential feature selection or that using the `Lasso` to select the features?  \n",
    "\n",
    "Assign your train and test error using the `model_selector_pipe` as `selector_train_mse` and `selector_test_mse` below.\n",
    "\n",
    "For more information and examples on `SelectFromModel` see [here](https://scikit-learn.org/stable/modules/feature_selection.html#select-from-model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selector_pipe = Pipeline([('poly_features', PolynomialFeatures(degree = 3, include_bias = False)),\n",
    "                                ('scaler', StandardScaler()),\n",
    "                                ('selector', SelectFromModel(Lasso())),\n",
    "                                    ('linreg', LinearRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f36419f78eb4822a",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "selector_train_mse = ''\n",
    "selector_test_mse = ''\n",
    "\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "model_selector_pipe.fit(auto_X_train, auto_y_train)\n",
    "selector_train_mse = mean_squared_error(auto_y_train, model_selector_pipe.predict(auto_X_train))\n",
    "selector_test_mse = mean_squared_error(auto_y_test, model_selector_pipe.predict(auto_X_test))\n",
    "### END SOLUTION\n",
    "\n",
    "# Answer check\n",
    "print(selector_train_mse)\n",
    "print(selector_test_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b56b71b7f554b647",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Further work could involve grid searching parameters of both the transformers and estimators, as well as including a `Ridge` regressor in the mix.  For now, you should be getting comfortable using the scikitlearn `Pipeline` object to combine transformers and estimators. This module introduced examples that can mitigate overfitting in a regression context.  It is important to note that not one strategy is always best for a modeling problem.  Instead, you should consider multiple approaches and let your goals for the model guide you to determine which model best suits your performance metric."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
