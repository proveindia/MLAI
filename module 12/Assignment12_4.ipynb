{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "-6G7P7X-sMev",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-da86c9541d2f942a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Required Assignment 12.4: KNN for Regression and Imputation\n",
    "\n",
    "**Expected Time = 60 minutes** \n",
    "\n",
    "**Total Points = 50** \n",
    "\n",
    "This activity extends the use of K Nearest Neighbors to the problem of regression.  While typically not as high performing in predictive models, the KNN model for regression can be an effective approach to imputing missing data.  You will explore both of these ideas using scikit-learn, where there exists the `KNeighborsRegressor` and the `KNNImputer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eoUFqpeSsMez",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9d3f95dd45662d4b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### Index\n",
    "\n",
    "- [Problem 1](#-Problem-1)\n",
    "- [Problem 2](#-Problem-2)\n",
    "- [Problem 3](#-Problem-3)\n",
    "- [Problem 4](#-Problem-4)\n",
    "- [Problem 5](#-Problem-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dUdzLI4isMez"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn import set_config\n",
    "set_config(\"figure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYtgtqOwsMe1",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-97f06c75d5247c34",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### The Data\n",
    "\n",
    "To begin, you will use a dataset accessed from the R languages DAAG package containing information on possums trapped at seven different sites in Australia.  It is loaded and displayed below.  Your regression task will be to predict the head size using the other features.  The training and testing data is created for you below as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x_RHuhWTsMe1"
   },
   "outputs": [],
   "source": [
    "possums_missing = pd.read_csv('data/possum.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possums_missing.info() #note the missing values -- we will drop these to begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possums = possums_missing.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WJrW4tgCsMe4",
    "outputId": "01d69053-aad1-4da9-b438-84728f9cf69c"
   },
   "outputs": [],
   "source": [
    "possums.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Q4Dd2BrsMe4"
   },
   "outputs": [],
   "source": [
    "X = possums.drop(['skullw', 'Pop'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g3QGyUs1sMe5"
   },
   "outputs": [],
   "source": [
    "y = possums.skullw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t6kVAyEJsMe5"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gt0dh_UCsMe5",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6dab76784f9d32bc",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 1\n",
    "\n",
    "#### A Basic Regression Pipeline\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "Use the `make_column_transformer` function to define a transformer instance named `transformer`. Apply a `OneHotEncoder` transformation with `drop = 'if_binary'` to the `sex` column. Transform the `remainder` columns using `StandardScaler()`.\n",
    "\n",
    "\n",
    "Next, build a basic regression pipeline with steps `transformer` and `knn` that binarizes the categorical feature  and feeds these into a `KNeighborsRegressor` with all default settings. Assign your pipeline to `knn_pipe`.\n",
    "\n",
    "Use the `fit` function to fit the pipeline to the training sets.\n",
    "\n",
    "Use the `predict` function on `knn_pipe` to make predictions on `X_test`. Assign the result to `preds`.\n",
    "\n",
    "Finally, use the `mean_squared_error` function to compute the MSE between `y_test` and `preds`. Assign the results to `test_mse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8yaFj2mDsMe6",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-157db529463735c0",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "outputId": "443c29ed-655e-4471-de19-accbc6422faf"
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "transformer = ''\n",
    "knn_pipe = ''\n",
    "\n",
    "\n",
    "test_mse = ''\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "transformer = make_column_transformer((OneHotEncoder(drop = 'if_binary'), ['sex']),\n",
    "                                     remainder = StandardScaler())\n",
    "knn_pipe = Pipeline([('transformer', transformer), ('knn', KNeighborsRegressor())])\n",
    "knn_pipe.fit(X_train, y_train)\n",
    "preds = knn_pipe.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, preds)\n",
    "### END SOLUTION\n",
    "\n",
    "# Answer check\n",
    "print(test_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NDZoEEJsMe7",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d7aaaa6ea389e19c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 2\n",
    "\n",
    "#### GridSearch the Pipeline\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "Define a dictionary `params`. The key of this dictionary will be `'knn__n_neighbors'`, the values will be equal to `range(1, len(y_test), 2)`.\n",
    "\n",
    "Use the `GridSearchCV` function to perform a grid search on `knn_pipe` with `param_grid` equal to `params`.\n",
    "\n",
    "Use the `fit` function to fit the pipeline to the training sets.\n",
    "\n",
    "Use the `best_params_` method on `knn_pipe` with argument `'knn__n_neighbors'`. Assign the result to `best_k` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RmOQVlXasMe7",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-59001bc6f4bf325f",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "outputId": "455c970b-9122-426d-ab1e-a3e31d1bdcee"
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "params = {}\n",
    "knn_grid = ''\n",
    "best_k = ''\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "params = {'knn__n_neighbors': range(1, len(y_test), 2)}\n",
    "knn_grid = GridSearchCV(knn_pipe, param_grid=params)\n",
    "knn_grid.fit(X_train, y_train)\n",
    "best_k = knn_grid.best_params_['knn__n_neighbors']\n",
    "### END SOLUTION\n",
    "\n",
    "# Answer check\n",
    "print(best_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__AH4vJ7sMe8",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9f1fc16f888a5bb8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 3\n",
    "\n",
    "#### Handling the missing data\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "Earlier, we dropped the rows containing missing data.  If we wanted to keep these rows for our model we need to make a decision about what values to fill in.  The `KNNImputer` uses the K Nearest Neighbor algorithm in order to determine this value.  Intuitively, you could see the argument for this where you use similar observations to stand in for the missing values.  \n",
    "\n",
    "```\n",
    "Each sample's missing values are imputed using the mean value from `n_neighbors` nearest neighbors found in the training set. Two samples are close if the features that neither is missing are close.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H58DCIHksMe8",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0d21b9cf5547c467",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Use the `make_column_transformer` function to define a transformer instance named `transformer`. Apply a `OneHotEncoder` transformation with `drop = 'if_binary'` to the `sex` column. Transform the `remainder` columns using `StandardScaler()`.\n",
    "\n",
    "\n",
    "Next, build a basic regression pipeline with steps `'transform'`, `'impute'`, and `'model'`. Assign `transformer` to `'transform'`, `KNNImputer()` to `'impute'`, and `KNeighborsRegressor()` to `'model'`.\n",
    "\n",
    "Use the `fit` function to fit the pipeline to the `X_train_missing` and `y_train_missing`.\n",
    "\n",
    "Use the `predict` function on `imputer_pipe` to make predictions on `X_test_missing`. Assign the result to `preds`.\n",
    "\n",
    "Finally, use the `mean_squared_error` function to compute the MSE between `y_test_missing` and `preds`. Assign the results to `test_mse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qPt7e-XosMe8"
   },
   "outputs": [],
   "source": [
    "X = possums_missing.drop(['skullw', 'Pop'], axis = 1)\n",
    "y = possums_missing.skullw\n",
    "X_train_missing, X_test_missing, y_train_missing, y_test_missing = train_test_split(X, y, random_state = 43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5xG1LWeNsMe8",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f233139fa3f2b102",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "outputId": "2eb15e79-ab12-485d-abf4-a02c83c5f115"
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "imputer_pipe = ''\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "transformer = make_column_transformer((OneHotEncoder(drop = 'if_binary'), ['sex']), \n",
    "                                     remainder = StandardScaler())\n",
    "imputer_pipe = Pipeline([('transform', transformer), ('impute', KNNImputer()), ('model', KNeighborsRegressor())])\n",
    "imputer_pipe.fit(X_train_missing, y_train_missing)\n",
    "preds = imputer_pipe.predict(X_test_missing)\n",
    "test_mse = mean_squared_error(y_test_missing, preds)\n",
    "### END SOLUTION\n",
    "\n",
    "# Answer check\n",
    "print(test_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTEzFtQWsMe9",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bee452fdbb535351",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 4\n",
    "\n",
    "#### Grid Searching the Pipeline\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "\n",
    "Define a dictionary `params`. The keys of this dictionary will be `'model__n_neighbors'` and `'impute__n_neighbors'` with values  `range(1, len(y_test), 2)` and `[1, 2, 3, 4, 5]`, respectively.\n",
    "\n",
    "Use the `GridSearchCV` function to perform a grid search on `imputer_pipe` with `param_grid` equal to `params`. Assign the result to `imputer_grid`.\n",
    "\n",
    "Use the `fit` function to fit `imputer_grid` to `X_train_missing` and `y_train_missing`.\n",
    "\n",
    "Use the `best_params_` method on `imputer_grid`. Assign the result to `best_ks` below.\n",
    "\n",
    "Use the `predict` functions on `imputer_grid` to calculate the predictions on `X_test_missing`. Assign the result to `preds`.\n",
    "\n",
    "Finally, use the `mean_squared_error` function to calculate the MSE between `y_test_missing` and `preds`. Assign\n",
    "the mean squared error to `imputer_mse` below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WANndPT3sMe9",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bfe70713b0b74b31",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "outputId": "9e6c4c3c-0fc0-4c74-931b-e00250ed63ec"
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "params = {}\n",
    "imputer_grid = ''\n",
    "best_ks = ''\n",
    "imputer_mse = ''\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "params = {'model__n_neighbors': range(1, len(y_test), 2),\n",
    "         'impute__n_neighbors': [1, 2, 3, 4, 5]}\n",
    "imputer_grid = GridSearchCV(imputer_pipe, param_grid=params)\n",
    "imputer_grid.fit(X_train_missing, y_train_missing)\n",
    "best_ks = imputer_grid.best_params_\n",
    "preds = imputer_grid.predict(X_test_missing)\n",
    "imputer_mse = mean_squared_error(y_test_missing, preds)\n",
    "### END SOLUTION\n",
    "\n",
    "# Answer check\n",
    "print(best_ks)\n",
    "print(imputer_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "grtE9O8VsMe9",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e0f52ee1297da5f9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 5\n",
    "\n",
    "#### Interpreting the model\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "Unlike linear regression, we have no parameters from the resulting model to investigate and understand the effect of increasing or decreasing certain features based on these coefficients.  All hope is not lost however, as you can simulate this behavior by running through different values of each feature and exploring how the predictions from the model change.\n",
    "\n",
    "This is the idea behind the `partial_dependence` function in scikit-learn.  Note that it works in a similar manner to the confusion matrix display from earlier.  For a deeper discussion/example of partial dependence plots see the user guide [here](https://scikit-learn.org/stable/modules/partial_dependence.html#partial-dependence). Below, the Partial Dependence plots for six features are plotted.  Which feature seems more important -- `hdlngth` or `footlgth` based on these plots.  Assign your response as a string to `ans5` below. \n",
    "\n",
    "Again, the big idea is the x-axis represents increasing values of the feature, and the y-values represent the predicted value of the target.  The code that produced the plots is shown below as well as the plot. \n",
    "\n",
    "```python\n",
    "from sklearn.inspection import PartialDependenceDisplay, partial_dependence\n",
    "fig, ax = plt.subplots(figsize = (20, 6))\n",
    "PartialDependenceDisplay.from_estimator(pipe, X, features = ['hdlngth', 'totlngth', 'footlgth', 'earconch', 'eye', 'chest'], ax = ax)\n",
    "ax.set_title('Partial Dependence Plots for 6 Features')\n",
    "```\n",
    "\n",
    "<center>\n",
    "    <img src = 'images/part_dep.png'/>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9SYCiWBmsMe-",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5a43e7a35dfafb02",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "outputId": "16df164c-47d7-4f52-d432-e1b3cffd6bfb"
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "ans5 = ''\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "ans5 = 'hdlngth'\n",
    "### END SOLUTION\n",
    "\n",
    "# Answer check\n",
    "print(ans5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvtOpzCCsMe-"
   },
   "source": [
    "In a similar way, you could understand the features and their importance in the case of KNN for classification through partial dependence plots -- another situation where after fitting the model you do not get parameters.  In the next module, you will explore a classification method called Logistic Regression that does solve classification problems and contains coefficients after fitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "colab": {
   "collapsed_sections": [],
   "name": "coding_assignment_12_7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
